{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import validation_curve, learning_curve,GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score,roc_curve,accuracy_score,make_scorer \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Задания_1_2.xlsx\", sheet_name=\"Training\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#смотрим информацию об сэте\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#вставялем пропущенные значения минимальными\n",
    "df['P2'].fillna(df['P2'].mean(), inplace=True)\n",
    "df['P3'].fillna(df['P3'].mean(), inplace=True)\n",
    "df['P8'].fillna(df['P8'].mean(), inplace=True)\n",
    "df['P16'].fillna(df['P16'].mean(), inplace=True)\n",
    "df['P25'].fillna(df['P25'].mean(), inplace=True)\n",
    "df['P29'].fillna(df['P29'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#датафрэйм сбалансирован\n",
    "df[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#посмотрим корреляцию данных\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(40,20));\n",
    "sns.heatmap(df.corr('spearman'), cmap='PuOr', annot=True, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df.corr()\n",
    "c=corr.abs().unstack()\n",
    "c[c == 1] = 0\n",
    "c=c.sort_values(ascending = False).drop_duplicates()\n",
    "tmp=c.head(10)\n",
    "tmp.sort_values(ascending = True)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#заменим сильно коррелируемые данные на их произведение и удалим их\n",
    "def prepare_df(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    df[\"P23_22\"]=df[\"P23\"]*df[\"P22\"]\n",
    "    df[\"P25_17\"]=df[\"P25\"]*df[\"P17\"]\n",
    "    df[\"P1_5\"]=df[\"P1\"]*df[\"P5\"]\n",
    "    df[\"P12_15\"]=df[\"P12\"]*df[\"P15\"]\n",
    "    df[\"P31_29\"]=df[\"P31\"]*df[\"P29\"]\n",
    "    df=df.drop(labels=['P1','P5','P12','P15','P17','P22','P23','P25','P29','P31'], axis=1)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data=data.corr()\n",
    "c=corr_data.abs().unstack()\n",
    "c[c == 1] = 0\n",
    "c=c.sort_values(ascending = False).drop_duplicates()\n",
    "tmp=c.head(10)\n",
    "tmp.sort_values(ascending = True)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Target'])\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разбиваем данные на тренировочные тестовые и валидационные данные\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=232)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=232)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём функцию  построения моделей используя GridsearchCV и вывода метрик на тенировочных данных\n",
    "\n",
    "def fit_classifier(model, X, y, parameters=None, scorer_metrics=None):\n",
    "\n",
    "    # Perform grid search on the classifier using scorer_metrics as the scoring method\n",
    "    grid_obj = GridSearchCV(estimator = model, param_grid = parameters, scoring=make_scorer(scorer_metrics), cv=5)\n",
    "\n",
    "    # Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "    grid_fit = grid_obj.fit(X, y)\n",
    "\n",
    "    # Get the estimator\n",
    "    model_estimator = grid_fit.best_estimator_\n",
    "\n",
    "    # Report the metrics scores on train data\n",
    "    model_estimator.fit(X, y)\n",
    "    y_pred = model_estimator.predict(X)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nModel performance on training set\\n------------------------\")\n",
    "    print(\"Final accuracy score on the training data: {:.4f}\".format(accuracy_score(y, y_pred)))\n",
    "    print(\"Final precision score on training data: {:.4f}\".format(precision_score(y, y_pred)))\n",
    "    print(\"Final Recall score on training data: {:.4f}\".format(recall_score(y, y_pred)))\n",
    "    print(\"Final ROC AUC score on training data: {:.4f}\".format(roc_auc_score(y, y_pred)))\n",
    "    print(\"\\n\")\n",
    "    print(\"The best parameters are: {}\".format(model_estimator))\n",
    "\n",
    "    return model_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказать тестовые данные и вывода метрик на тестовых данных\n",
    "def classifier_test(model_fit, X, y):\n",
    "    y_pred = model_fit.predict(X)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nModel performance on test set\\n------------------------\")\n",
    "    print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y, y_pred)))\n",
    "    print(\"Final precision score on testing data: {:.4f}\".format(precision_score(y, y_pred)))\n",
    "    print(\"Final Recall score on testing data: {:.4f}\".format(recall_score(y, y_pred)))\n",
    "    print(\"Final ROC AUC score on testing data: {:.4f}\".format(roc_auc_score(y, y_pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построение графика ROC\n",
    "def roc_curve_plot(model, X, y,label=None):\n",
    "    \n",
    "    y_score = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    \n",
    "    roc = roc_curve(y, y_score)\n",
    "    \n",
    "    plt.plot(roc[0], roc[1], label=label)\n",
    "    plt.plot([0,1],[0,1], 'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    \n",
    "    roc_score = auc(roc[0],roc[1])\n",
    "    print('AUC score of %s is %.4f.' % (label, roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "def gain_plot(lift_input, label=None):\n",
    "    plt.plot(lift_input[1], lift_input[0], label=label)\n",
    "    plt.plot([0,1],[0,1], 'k--') # reference line for random model\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('Population%')\n",
    "    plt.ylabel('Subscribe%')\n",
    "    # calculate area under curve\n",
    "    AUC = auc(lift_input[1], lift_input[0], reorder=False)\n",
    "    print('AUC score of %s is %.4f.' % (label, AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим наилучшую модель для логистической регресии \n",
    "parameters_LR = {'C': [0.0001,0.0003, 0.0005], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "model_LR = fit_classifier(LogisticRegression(random_state=18), X_train, y_train, \n",
    "                          parameters=parameters_LR, scorer_metrics=recall_score)\n",
    "model_LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель логистической регрессии на тестовом датасэте\n",
    "y_test_LR = classifier_test(model_LR, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  строим наилучшую модель для random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create the parameters list\n",
    "parameters_RF = {'max_depth': [2,5,7,10], 'min_samples_leaf': [2,3,5,7,10], 'min_samples_split': [2,3,5,10]}\n",
    "\n",
    "model_RF = fit_classifier(RandomForestClassifier(random_state=18), X_train, y_train, \n",
    "                          parameters=parameters_RF, scorer_metrics=recall_score)\n",
    "model_RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest модель на тестовом датасэте\n",
    "y_test_RF = classifier_test(model_RF, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим наилучшую модель для DecisionTree\n",
    "parameters_DT = {'max_depth': [7,8,9],\n",
    "                 'min_samples_leaf': [2,3,4],\n",
    "                 'min_samples_split': [2,3,4]}\n",
    "\n",
    "model_DT = fit_classifier(DecisionTreeClassifier(random_state=44), X_train, y_train, \n",
    "                          parameters=parameters_DT, scorer_metrics=recall_score)\n",
    "model_DT.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree модель на тестовом датасэте\n",
    "y_test_DT = classifier_test(model_DT, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построение ROC кривых для всех моделей на тренировочном датасэте\n",
    "roc_curve_plot(model_LR, X_train, y_train, label='Logistic Regression')\n",
    "roc_curve_plot(model_DT, X_train, y_train, label='Decision Tree')\n",
    "roc_curve_plot(model_RF, X_train, y_train, label='Random Forest')\n",
    "plt.title('ROC Curves on Train Set')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построение ROC кривых для всех моделей на тестовом датасэте\n",
    "roc_curve_plot(model_LR, X_test, y_test, label='Logistic Regression')\n",
    "roc_curve_plot(model_DT, X_test, y_test, label='Decision Tree')\n",
    "roc_curve_plot(model_RF, X_test, y_test, label='Random Forest')\n",
    "plt.title('ROC Curves on Test Set')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_plot(model_LR, X_val, y_val, label='Logistic Regression')\n",
    "roc_curve_plot(model_DT, X_val, y_val, label='Decision Tree')\n",
    "roc_curve_plot(model_RF, X_val, y_val, label='Random Forest')\n",
    "plt.title('ROC Curves on Valid Set')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RF.fit(X, y)\n",
    "\n",
    "pred = model_RF.predict_proba(X)[:, 1]\n",
    "print(\"Total ROC AUC: %.2f\" % roc_auc_score(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция построения матрицы ошибок\n",
    "def show_confusion_matrix(X, y, clf, threshold=0.7):\n",
    "    pred = clf.predict_proba(X)[:, 1]\n",
    "    pred = [1 if p >= threshold else 0 for p in pred]\n",
    "    cm = pd.DataFrame(confusion_matrix(y, pred),\n",
    "        index=[\"Real 0\", \"Real 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "    print(\"Threshold = %.2f\" % threshold)\n",
    "    print(\"Accuracy score: %.1f%%\" % (100 * accuracy_score(y, pred)))\n",
    "    print(\"Confusion matrix:\")\n",
    "    display(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(X, y, model_RF, threshold=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание словаря для переменных\n",
    "def importances_dict(columns, model):\n",
    "    importances = dict()\n",
    "    for col, importance in zip(columns, model.feature_importances_):\n",
    "        importances[col] = importance.round(2)\n",
    "    return importances\n",
    "# построение графика важности переменных\n",
    "def plot_importances(importances):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    pd.Series(importances).plot(kind='barh', ax=ax)\n",
    "    plt.title(\"Важность переменных\")\n",
    "    plt.grid(axis=\"x\")\n",
    "    plt.show()\n",
    "    \n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plot_importances(importances_dict(X.columns, model_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_excel(\"Задания_1_2.xlsx\", sheet_name=\"Validate\")\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del valid_df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df=prepare_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_RF.predict_proba(X).T[1]\n",
    "\n",
    "pred = pd.DataFrame({ \"prediction\": pred})\n",
    "pred.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv(\"data/validate_scores.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd03a210555fd971971affe42310260fccebbfe3ae6184d5e7c72f1b3effed09"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('a_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
